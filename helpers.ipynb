{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile PLW.py\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def myPLWLS(y,x,attr1=x,mse=1,costfn=1):\n",
    "    print('mse=',mse,'costfn=',costfn)\n",
    "    if np.allclose(np.ones([attr1.shape[0],]),attr1[:,0]):#fix problems with differencing column of 1's\n",
    "        attr=attr1[:,1:]\n",
    "    else: attr=attr1\n",
    "    n,k=np.shape(x)\n",
    "    j=np.shape(attr)[1]+1 #plus 1 to go with the column of 1's added after differencing\n",
    "    attr_tile=np.tile(attr,(n,1,1))# stack the attributes matrix (nxk array) n times\n",
    "    diffstack=np.transpose(attr_tile,(1,0,2))-attr_tile#transpose the z and x dimensions (listed as z,x,y)\n",
    "    diffstack=np.concatenate((np.ones([n,n,1]),diffstack), axis=2) # add a column of 1's to all the differences\n",
    "    wt_try0=np.zeros([j,])\n",
    "    wt_try0[0]=1\n",
    "    return minimize(PLWLStryMSE,wt_try0,args=(y,x,diffstack,mse,costfn),method='Nelder-Mead')\n",
    "\n",
    "def Attr_To_Diffstck(attr):\n",
    "    n,j=np.shape(attr); j+=1 #plus 1 to go with the column of 1's added after differencing\n",
    "    attr_tile=np.tile(attr,(n,1,1))# stack the attributes matrix (nxk array) n times\n",
    "    diffstack=np.transpose(attr_tile,(1,0,2))-attr_tile#transpose the z and x dimensions (listed as z,x,y)\n",
    "    return np.concatenate((np.ones([n,n,1]),diffstack), axis=2)\n",
    "\n",
    "\n",
    "def Reg_wts_stack(corrparams,diffstack):\n",
    "    #return np.sum(((2**corrparams*diffstack)**2),axis=2)**-0.5)\n",
    "    #weighted, squared, summed across wt parameter, j, rooted and flipped\n",
    "    regwt_stack=np.sum(np.exp((corrparams*diffstack)**2),axis=2)**(-0.5)\n",
    "    return regwt_stack/(np.sum(regwt_stack**2)**.5/regwt_stack.shape[0])#normalize the weights seems to help convergence\n",
    "    \n",
    "def PLWLScastMSE(corrparams,attr1,y,x):\n",
    "    #use corrparams, correspondence attributes, known y and x to calculate mse for the estimator/corrparams\n",
    "    if np.allclose(np.ones([attr1.shape[0],]),attr1[:,0]):#fix problems with differencing column of 1's\n",
    "        attr=attr1[:,1:]\n",
    "    else: attr=attr1\n",
    "    n,k=np.shape(x)\n",
    "    regwt_stack=Reg_wts_stack(corrparams,Attr_To_Diffstck(attr))\n",
    "    \n",
    "    x_wt_stck=np.tile(regwt_stack.reshape(n,n,1),(1,1,k))*np.tile(x,(n,1,1))\n",
    "    y_wt_stck=regwt_stack.reshape(n,n,1)*np.tile(y.reshape(n,1),(n,1,1))\n",
    "    x_wt_stckT=np.transpose(x_wt_stck,(0,2,1))#transpose X aross the stack for x'x\n",
    "    Qi=np.linalg.inv(x_wt_stckT@x_wt_stck)\n",
    "    Bstack=Qi@x_wt_stckT@y_wt_stck\n",
    "    #print('first 5 params=',Bstack[0:5,:,:])\n",
    "    \n",
    "                             \n",
    "    ystack=np.tile(y.reshape(n,1),(n,1,1))\n",
    "    xstack=np.tile(x,(n,1,1))\n",
    "    err=(ystack-xstack@Bstack)#errors are not calculated using weighted y and x\n",
    "    \n",
    "    yhat_stack=xstack@Bstack                         \n",
    "    yhats=yhat_stack.diagonal().diagonal()\n",
    "    err_diag=err.diagonal().diagonal()\n",
    "    mse=np.sum(err_diag**2)/err_diag.size\n",
    "    '''p=figure()\n",
    "    p.circle(x[:,1],y)\n",
    "    p.square(x[:,1],yhats)\n",
    "    p.show()'''\n",
    "    #print('mse_cast=',mse)\n",
    "    return mse\n",
    "    \n",
    "\n",
    "    \n",
    "def PLWLStryMSE(corrparams,y,x,diffstack,MSE=1,costfn=1):\n",
    "    #print(x.shape)\n",
    "    print('corrparams=',corrparams)\n",
    "    n,k=np.shape(x)\n",
    "    j=np.shape(corrparams)[0]\n",
    "    #print(k,j)\n",
    "    regwt_stack=Reg_wts_stack(corrparams,diffstack)\n",
    "    x_wt_stck=np.tile(regwt_stack.reshape(n,n,1),(1,1,k))*np.tile(x,(n,1,1))#the stack of weights is broadcast and multiplied by stack of x.(z,x,y) indexing\n",
    "    y_wt_stck=regwt_stack.reshape(n,n,1)*np.tile(y.reshape(n,1),(n,1,1))#the stack of weights is broadcast and multiplied by stack of y.\n",
    "    x_wt_stckT=np.transpose(x_wt_stck,(0,2,1))#transpose X aross the stack\n",
    "    Qi=np.linalg.inv(x_wt_stckT@x_wt_stck)\n",
    "    #print(y_wt_stck.shape)\n",
    "    Bstack=Qi@x_wt_stckT@y_wt_stck #use weighted data to calculate gls parameters\n",
    "    #print('first params=',Bstack[0,:,:])\n",
    "    ystack=np.tile(y.reshape(n,1),(n,1,1))\n",
    "    xstack=np.tile(x,(n,1,1))\n",
    "    err=(ystack-xstack@Bstack)#errors are not calculated using weighted y and x\n",
    "    if costfn==1: cost_err=err**2-regwt_stack**(-2) #squared error prediction\n",
    "    if costfn==2: cost_err=err\n",
    "    corr_err=err**2-regwt_stack**(-2)\n",
    "    if MSE==1:\n",
    "        out=np.sum(cost_err**2)/cost_err.size\n",
    "        print('{}MSE all='.format(costfn),out)\n",
    "        return out\n",
    "    if MSE==2:\n",
    "        wt_cost_err=(cost_err)*regwt_stack\n",
    "        out=np.sum(wt_cost_err**2)/wt_cost_err.size\n",
    "        print('{}MSE wt_all='.format(costfn),out)\n",
    "        return out\n",
    "    if MSE==3:\n",
    "        diag_err=cost_err.diagonal().diagonal()\n",
    "        print('diag',diag_err.shape)\n",
    "        out=np.sum(diag_err**2)/diag_err.size\n",
    "        print('{}MSE n_only='.format(costfn),out)\n",
    "        return out\n",
    "    \n",
    "def PLWLSpredict(corrparams,attr1,y,x,attr1_val,y_val,x_val):\n",
    "    #use corrparams, correspondence attributes, known y and x to calculate mse for the estimator/corrparams\n",
    "    if np.allclose(np.ones([attr1.shape[0],]),attr1[:,0]):#fix problems with differencing column of 1's\n",
    "        attr=attr1[:,1:]\n",
    "    else: attr=attr1\n",
    "    if np.allclose(np.ones([attr1_val.shape[0],]),attr1_val[:,0]):#fix problems with differencing column of 1's\n",
    "        attr_val=attr1_val[:,1:]    \n",
    "    else: attr_val=attr1_val\n",
    "    n,k=np.shape(x)\n",
    "    nv,k1=np.shape(x_val)\n",
    "    j=np.shape(attr)[1]+1 #plus 1 to go with the column of 1's added after differencing\n",
    "    ctr_attr_stck=np.tile(attr_val,(n,1,1))# stack the centered attributes matrix (nxk array) n times\n",
    "    diffstack0=np.transpose(ctr_attr_stck,(1,0,2))-np.tile(attr,(nv,1,1))#transpose the z and x dimensions (listed as z,x,y)\n",
    "    diffstack=np.concatenate((np.ones([nv,n,1]),diffstack0), axis=2) ##so diffstack is shape nv,n,j\n",
    "    \n",
    "    regwt_stack=Reg_wts_stack(corrparams,diffstack)\n",
    "    x_wt_stck=np.tile(regwt_stack.reshape(nv,n,1),(1,1,k))*np.tile(x,(nv,1,1))\n",
    "    y_wt_stck=regwt_stack.reshape(nv,n,1)*np.tile(y.reshape(n,1),(nv,1,1))\n",
    "    x_wt_stckT=np.transpose(x_wt_stck,(0,2,1))#transpose X aross the stack for x'x\n",
    "    Qi=np.linalg.inv(x_wt_stckT@x_wt_stck)\n",
    "    Bstack=Qi@x_wt_stckT@y_wt_stck\n",
    "    #print('first 5 params=',Bstack[0:5,:,:])\n",
    "                                 \n",
    "    ystack=np.tile(y_val.reshape(nv,1),(nv,1,1))\n",
    "    xstack=np.tile(x_val,(nv,1,1))\n",
    "    err=(ystack-xstack@Bstack)#errors are not calculated using weighted y and x\n",
    "    \n",
    "    yhat_stack=xstack@Bstack                         \n",
    "    yhats=yhat_stack.diagonal().diagonal()\n",
    "    err_diag=err.diagonal().diagonal()\n",
    "    mse=np.sum(err_diag**2)/err_diag.size\n",
    "    '''p=figure()\n",
    "    p.circle(x[:,1],y)\n",
    "    p.square(x[:,1],yhats)\n",
    "    p.show()'''\n",
    "    #print('mse_cast=',mse)\n",
    "    return mse    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile reg.py\n",
    "import numpy as np\n",
    "\n",
    "class reg():\n",
    "    \n",
    "\n",
    "        def myreg(self,y,x):\n",
    "            # a simple regression that prints its own mse and returns estimated parameters\n",
    "            q=np.linalg.inv(np.matmul(x.T,x))\n",
    "            self.b=np.matmul(np.matmul(q,x.T),y)\n",
    "            self.mse=np.sum((y-x@self.b)**2)/y.shape[0]\n",
    "\n",
    "        def myregstdz(self,y,x): \n",
    "            [n,k]=np.shape(x)\n",
    "            onecol=0\n",
    "            xfull=x\n",
    "            if np.allclose(np.ones([n,1]),x[:,0]):#fix problems with standardizing column of 1's\n",
    "                x=x[:,1:]\n",
    "                onecol=1;\n",
    "            xmean=np.mean(x,axis=0)\n",
    "            xvar=np.var(x,axis=0,ddof=1)#subtracting xmean isn't necessary, as it happens anyways\n",
    "            onevec=np.ones([n,1])\n",
    "            yvar=np.var(y, ddof=1)\n",
    "            sy=np.power(yvar,.5)\n",
    "            sx=np.power(xvar,.5)\n",
    "            ymean=np.mean(y)        \n",
    "            xstdz=(1/sx)*(x-xmean)\n",
    "            ystdz=(1/sy)*(y-ymean)\n",
    "            Q=np.linalg.inv(np.matmul(xstdz.T,xstdz))\n",
    "            bhat1=np.matmul(np.matmul(Q,xstdz.T),ystdz)\n",
    "            bhat=sy*bhat1*(1/sx)#undo beta coef. by multiplying each by Sy/Sx\n",
    "            if onecol==1:\n",
    "                bhat_int=ymean-np.sum(sy*xmean*bhat1*(1/sx))\n",
    "                bhat=np.concatenate([[bhat_int],bhat])\n",
    "            self.mse=np.sum((y-xfull@bhat)**2)/y.shape[0]\n",
    "            self.b=bhat\n",
    "\n",
    "            def myregpredict(y,x,yval,xval):\n",
    "    # a simple regression that prints its own mse and returns estimated parameters\n",
    "    q=np.linalg.inv(np.matmul(x.T,x))\n",
    "    bhat=np.matmul(np.matmul(q,x.T),y)\n",
    "    mse=np.sum((yval-xval@bhat)**2)/yval.shape[0]\n",
    "    #print('mse=',mse)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data_gen.py\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class data_gen():\n",
    "    '''generates numpy arrays of random training or validation for model: y=xb+e or variants\n",
    "    '''\n",
    "    def __init__(self, xtrain_size=(200,5), ftype='linear', xval_size='same', sparsity=0, xvar=1, xmean=0, evar=1, betamax=10):\n",
    "        k_1=200\n",
    "        n=5\n",
    "        self.xtrain=\n",
    "        \n",
    "    \n",
    "        xtall=xmean\n",
    "        xwide=xvar\n",
    "        spreadx=np.random.randint(xwide, size=(1,k_1))+1#random row vector to multiply by each random column of x to allow s.d. upto 5\n",
    "        shiftx=np.random.randint(0,xtall, size=(1,k_1))-xtall/2#random row vector to add to each column of x\n",
    "        randx=np.random.randn(n,k_1)\n",
    "        self.x = np.concatenate((np.ones((n,1)),shiftx+spreadx*randx),axis=1)\n",
    "        #generate error~N(0,1)\n",
    "        self.e=np.random.randn(n)*evar**.5\n",
    "        \n",
    "\n",
    "        #make beta integer, non-zero\n",
    "        self.b=(np.random.randint(betamax, size=(k_1+1,))+1)*(2*np.random.randint(2, size=(k_1+1,))-np.ones(k_1+1,)) #if beta is a random integer, it could be 0\n",
    "        #make a simple y for testing\n",
    "        self.y=np.matmul(self.x, self.b)+self.e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
